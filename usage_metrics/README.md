# Usage Metrics Dagster Pipeline
This project contains a dagster repository that extracts, cleans and loads PUDL's usage metrics into a data warehouse for analysis.

### Project Structure
This is the project structure generated by the [dagster cli](https://docs.dagster.io/getting-started/create-new-project#create-a-new-project):

| Name                     | Description                                                                       |
| ------------------------ | --------------------------------------------------------------------------------- |
| `README.md`              | A description and guide for this code repository                                  |
| `workspace.yaml`         | A file that specifies the location of the user code for Dagit and the Dagster CLI |
| `src/usage_metrics/`     | A Python directory that contains code for your Dagster repository                 |
| `usage_metrics_tests/`   | A Python directory that contains tests for `usage_metrics`                        |
| `setup.py`               | A build script with Python package dependencies for this code repository          |

# Setup
## Conda Environment
Make sure you have [conda installed](https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html). Once conda is installed, run these commands from the `business` directory:
```
conda update conda
conda env create --name business-dev --file environment.yml
conda activate business-dev
```

## Environment Variables
The ETL uses [ipinfo](https://ipinfo.io/) to geocode ip addresses. You need to obtain an ipinfo API token and store it in the `IPINFO_TOKEN` environment variable.

Dagster stores run logs and caches in a directory stored in the `DAGSTER_HOME` environment variable. The `usage_metrics/dagster_home/dagster.yaml` file contains configuration for the dagster instance. **Note:** The `usage_metrics/dagster_home/storage` directory could grow to become a couple GBs because all op outputs for every run are stored there.  You can read more about the dagster_home directory in the [dagster docs](https://docs.dagster.io/deployment/dagster-instance#default-local-behavior).

To set these environment variables, run these commands **from the `business` directory:**
```
conda activate business-dev
conda env config vars set IPINFO_TOKEN=‘your_api_key_here’
conda env config vars set DAGSTER_HOME="$(pwd)/usage_metrics/dagster_home/"
conda activate business-dev
```

## Service Account Key
The ETL needs access to BigQuery, so you'll need to acquire the `datasette-log-viewer` service account key and place it in the `/business/usage_metrics/bigquery-service-account-key.json` file. **This file is a password for the service account so make sure it is stored securely!**

## Git Pre-commit Hooks
Git hooks let you automatically run scripts at various points as you manage your source code. “Pre-commit” hook scripts are run when you try to make a new commit. These scripts can review your code and identify bugs, formatting errors, bad coding habits, and other issues before the code gets checked in. This gives you the opportunity to fix those issues before publishing them.

To make sure they are run before you commit any code, you need to enable the pre-commit hooks scripts with this command:

```
pre-commit install
````

The scripts that run are configured in the .pre-commit-config.yaml file.

## Deploy Dagster Locally
Now the environment is all set up and we can start up dagster!

### Dagster Daemon
In one terminal window start the dagster-daemon by running these commands:

```
conda activate business-dev
cd usage_metrics
dagster-daemon run
```

The [dagster-daemon](https://docs.dagster.io/deployment/dagster-daemon) is a long-running service required  for  schedules, sensors and run queueing. The usage metrics ETL requires the daemon because the data is processed in partitions. Dagster kicks off individual runs for each [partition](https://docs.dagster.io/concepts/partitions-schedules-sensors/partitions) which are sent to a queue managed by the dagster-daemon.

### Dagit
In another terminal window, start the [dagit UI](https://docs.dagster.io/concepts/dagit/dagit) by running these commands:

```
conda activate business-dev
cd usage_metrics
dagit
```

This will launch dagit at [`http://localhost:3000/`](http://localhost:3000/). If you have another service running on port 3000 you can change the port by running:

```
dagit -p {another_cool_port}
```

Dagit allows you to kick off [`backfills`](https://docs.dagster.io/concepts/partitions-schedules-sensors/backfills) and run partitions with specific configuration.

## Run the ETL
You can run the ETL via the dagit UI or the [dagster CLI](https://docs.dagster.io/_apidocs/cli). Running backfills will populate the `usage_metrics/data/usage_metrics.db` sqlite database with clean data from our datasette logs.

### CLI
To run a complete backfill run:

```
dagster job backfill --all process_datasette_logs_locally
```

from `business/usage_metrics` with the `business-dev` conda env activated.

### Dagit UI
To run a a complete backfill from the Dagit UI go to the [partitions tab](http://localhost:3000/workspace/usage_metrics@usage_metrics/jobs/process_datasette_logs_locally/partitions). Then click on the "Launch Backfill" button in the upper left corner of the window. This should bring up a new window with a list of partitions.  Click "Select All" and then click the "Submit" button. This will submit a run for each partition. You can follow the runs on the ["Runs" tab](http://localhost:3000/instance/runs).

### Database
The ETL creates a sqlite database called `usage_metrics.db` in the `usage_metrics/data/` directory. It currently contains one table called `datasette_request_logs`. Each partitioned ETL run will append the new cleaned datasette logs to `datasette_request_logs`. A primary key constraint error will be thrown if you rerun the ETL for a partition. If you want to recreate the entire database just delete the sqlite database and rerun the ETL.

### IP Geocoding with ipinfo
I've been using [ipinfo](https://ipinfo.io/) for geocoding the user ip addresses. We get 50k free API requests a month. The `usage_metrics.helpers.geocode_ip()` function using [joblib](https://joblib.readthedocs.io/en/latest/#main-features) to cache API calls so we don't call the API multiple times for a single ip address. The first time you run the ETL no API calls will be cached so the `geocode_ips()` op will take a while to complete.

## Add new data sources
To add a new data source to the dagster repo, add new modules to the `usage_metrics/jobs/` and `usage_metrics/ops/` directories. Then add the new job to the usage_metrics.repository.usage_metrics() repo.

## Review the metrics
To review the datasette log metrics, launch jupyter lab and run the `notebooks/datasette_analysis.ipynb` notebook.
