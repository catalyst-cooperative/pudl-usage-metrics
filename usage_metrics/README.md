# Usage Metrics Dagster Pipeline
This project contains a dagster repository that extracts, cleans and loads PUDL's usage metrics into a data warehouse for analysis.

### Project Structure
This is the project structure generated by the [dagster cli](https://docs.dagster.io/getting-started/create-new-project#create-a-new-project):

| Name                     | Description                                                                       |
| ------------------------ | --------------------------------------------------------------------------------- |
| `README.md`              | A description and guide for this code repository                                  |
| `setup.py`               | A build script with Python package dependencies for this code repository          |
| `workspace.yaml`         | A file that specifies the location of the user code for Dagit and the Dagster CLI |
| `usage_metrics/`       | A Python directory that contains code for your Dagster repository                 |
| `usage_metrics_tests/` | A Python directory that contains tests for `usage_metrics`                      |

# Setup
## Conda Environment
Make sure you have [conda installed](https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html). Once conda is installed, run from the business directory:
```
conda env create --name business-dev --file environment.yml
```
Then activate the environment:
```
conda activate business-dev
```
This conda environment has python, pip and pre-commit installed in it. This env is just for running pre-commits, the actual ETL development happens in docker.

## Docker
[Install docker](https://docs.docker.com/get-docker/). Once you have docker installed, make sure it is running.

Now we can build the docker images by running:
```
make build
```
This command creates a docker image and installs all the packages in `requirements.txt` so it will take a couple minutes to complete.

If you get this error:
```
Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
```
during this step it means docker is not running.

## Environment Variables
Once the image is created we need to set some environment variables. First, make a new file in the repo root directory called `.env` and enter these two lines to configure local ports:
```
DAGIT_PORT=3001
POSTGRES_PORT=8085
JUPYTER_PORT=8890
```
These ports can be changed if you have other services running on these ports.

Second, make a copy of `default.env` and call it `local.env`. Follow the instructions inside to set up API key access. `local.env` contains environment variables that can be accessed within the docker container. You can read more about docker environment variables [here](https://docs.docker.com/compose/environment-variables/).

## Service Account Key
The docker container needs access to BigQuery, so you'll need to acquire the `datasette-log-viewer` service account key and place it in the `/business/usage_metrics/bigquery-service-account-key.json` file. **This file is a password for the service account so make sure it is stored securely!**

## Run the ETL
Now that weâ€™ve built the images and set the environment variables run:
```
make run_elt
```
to run the ELT and load the data to the postgres database running on `POSTGRES_PORT`.

You can also launch the ELT via dagit. To launch dagit, run:
```
make launch
```
This will launch dagit on http://localhost:3001/ and jupyter lab at http://localhost:8890/.