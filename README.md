# Usage Metrics Dagster Pipeline

This project contains a dagster repository that extracts, cleans and loads PUDL's usage metrics into a data warehouse for analysis.

### Project Structure

This is the project structure generated by the [dagster cli](https://docs.dagster.io/getting-started/create-new-project#create-a-new-project):

| Name                   | Description                                                                       |
| ---------------------- | --------------------------------------------------------------------------------- |
| `README.md`            | A description and guide for this code repository                                  |
| `workspace.yaml`       | A file that specifies the location of the user code for Dagit and the Dagster CLI |
| `src/usage_metrics/`   | A Python directory that contains code for your Dagster repository                 |
| `usage_metrics_tests/` | A Python directory that contains tests for `usage_metrics`                        |
| `setup.py`             | A build script with Python package dependencies for this code repository          |

# Setup

## Conda Environment

We use the conda package manager to specify and update our development environment. We recommend using [miniconda](https://docs.conda.io/en/latest/miniconda.html) rather than the large pre-defined collection of scientific packages bundled together in the Anaconda Python distribution. You may also want to consider using [mamba](https://github.com/mamba-org/mamba) – a faster drop-in replacement for conda written in C++.

```
conda update conda
conda env create --name pudl-usage-metrics --file environment.yml
conda activate pudl-usage-metrics
```

## Environment Variables

The ETL uses [ipinfo](https://ipinfo.io/) to geocode ip addresses. You need to obtain an ipinfo API token and store it in the `IPINFO_TOKEN` environment variable.

Dagster stores run logs and caches in a directory stored in the `DAGSTER_HOME` environment variable. The `usage_metrics/dagster_home/dagster.yaml` file contains configuration for the dagster instance. **Note:** The `usage_metrics/dagster_home/storage` directory could grow to become a couple GBs because all op outputs for every run are stored there. You can read more about the dagster_home directory in the [dagster docs](https://docs.dagster.io/deployment/dagster-instance#default-local-behavior).

To set these environment variables, run these commands:

```
conda activate pudl-usage-metrics
conda env config vars set IPINFO_TOKEN="{your_api_key_here}"
conda env config vars set DAGSTER_HOME="$(pwd)/dagster_home/"
conda activate pudl-usage-metrics
```

## Google Cloud Permissions

Ask the project admin of the `catalyst-cooperative-pudl` to add your email to the `pudl-usage-metrics-etl` group to acquire the adequate permissions to run the ETL locally. Once you have been added to the group, run:

```
gcloud auth application-default login
```

in your terminal. This command will prompt you to log in to your gmail account. Once completed, your Google credentials will be available in your environment.

## Git Pre-commit Hooks

Git hooks let you automatically run scripts at various points as you manage your source code. “Pre-commit” hook scripts are run when you try to make a new commit. These scripts can review your code and identify bugs, formatting errors, bad coding habits, and other issues before the code gets checked in. This gives you the opportunity to fix those issues before publishing them.

To make sure they are run before you commit any code, you need to enable the pre-commit hooks scripts with this command:

```
pre-commit install
```

The scripts that run are configured in the .pre-commit-config.yaml file.

## Deploy Dagster Locally

Now the environment is all set up and we can start up dagster!

### Dagster Daemon

In one terminal window start the dagster-daemon by running these commands:

```
conda activate pudl-usage-metrics
dagster-daemon run
```

The [dagster-daemon](https://docs.dagster.io/deployment/dagster-daemon) is a long-running service required for schedules, sensors and run queueing. The usage metrics ETL requires the daemon because the data is processed in partitions. Dagster kicks off individual runs for each [partition](https://docs.dagster.io/concepts/partitions-schedules-sensors/partitions) which are sent to a queue managed by the dagster-daemon.

### Dagit

In another terminal window, start the [dagit UI](https://docs.dagster.io/concepts/dagit/dagit) by running these commands:

```
conda activate pudl-usage-metrics
dagit
```

This will launch dagit at [`http://localhost:3000/`](http://localhost:3000/). If you have another service running on port 3000 you can change the port by running:

```
dagit -p {another_cool_port}
```

Dagit allows you to kick off [`backfills`](https://docs.dagster.io/concepts/partitions-schedules-sensors/backfills) and run partitions with specific configuration.

## Run the ETL

You can run the ETL via the dagit UI or the [dagster CLI](https://docs.dagster.io/_apidocs/cli). Running backfills will populate the `usage_metrics/data/usage_metrics.db` sqlite database with clean data from our datasette logs.

### CLI

To run a complete backfill run:

```
dagster job backfill --all process_datasette_logs_locally
```

from `business/usage_metrics` with the `pudl-usage-metrics` conda env activated.

### Dagit UI

To run a a complete backfill from the Dagit UI go to the [partitions tab](http://localhost:3000/workspace/usage_metrics@usage_metrics/jobs/process_datasette_logs_locally/partitions). Then click on the "Launch Backfill" button in the upper left corner of the window. This should bring up a new window with a list of partitions. Click "Select All" and then click the "Submit" button. This will submit a run for each partition. You can follow the runs on the ["Runs" tab](http://localhost:3000/instance/runs).

### Database

The ETL creates a sqlite database called `usage_metrics.db` in the `usage_metrics/data/` directory. It currently contains one table called `datasette_request_logs`. Each partitioned ETL run will append the new cleaned datasette logs to `datasette_request_logs`. A primary key constraint error will be thrown if you rerun the ETL for a partition. If you want to recreate the entire database just delete the sqlite database and rerun the ETL.

### IP Geocoding with ipinfo

I've been using [ipinfo](https://ipinfo.io/) for geocoding the user ip addresses. We get 50k free API requests a month. The `usage_metrics.helpers.geocode_ip()` function using [joblib](https://joblib.readthedocs.io/en/latest/#main-features) to cache API calls so we don't call the API multiple times for a single ip address. The first time you run the ETL no API calls will be cached so the `geocode_ips()` op will take a while to complete.

## Add new data sources

To add a new data source to the dagster repo, add new modules to the `usage_metrics/jobs/` and `usage_metrics/ops/` directories. Then add the new job to the usage_metrics.repository.usage_metrics() repo.

## Review the metrics

To review the datasette log metrics, launch jupyter lab and run the `notebooks/datasette_analysis.ipynb` notebook.
